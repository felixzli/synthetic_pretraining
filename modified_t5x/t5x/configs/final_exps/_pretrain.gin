from __gin__ import dynamic_registration

import __main__ as train_script
import seqio
from t5x import gin_utils
from t5x import partitioning
from t5x import utils
from t5x import trainer
from t5x.examples.t5 import network

# Must be overridden
MODEL_DIR = %gin.REQUIRED
MIXTURE_OR_TASK_NAME = %gin.REQUIRED
TASK_FEATURE_LENGTHS = %gin.REQUIRED
TRAIN_STEPS = %gin.REQUIRED
LEARNING_RATE = %gin.REQUIRED
EVAL_PERIOD = %gin.REQUIRED
PERIOD = %gin.REQUIRED
PACK = %gin.REQUIRED
FACTORS = %gin.REQUIRED
BATCH_SIZE = %gin.REQUIRED
DROPOUT_RATE = %gin.REQUIRED

# Sometimes overridden
EVAL_STEPS = 20

Z_LOSS = 0.0
LABEL_SMOOTHING = 0.0
USE_CACHED_TASKS = False # was originally True

# DEPRECATED: Import the this module in your gin file.
MIXTURE_OR_TASK_MODULE = None


network.T5Config:
  dropout_rate = %DROPOUT_RATE
  
train_script.train:
  model = %MODEL  # imported from separate gin file
  model_dir = %MODEL_DIR
  train_dataset_cfg = @train/utils.DatasetConfig()
  train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()
  infer_eval_dataset_cfg = None
  checkpoint_cfg = @utils.CheckpointConfig()
  partitioner = @partitioning.ModelBasedPjitPartitioner()
  trainer_cls = @trainer.Trainer
  total_steps = %TRAIN_STEPS
  eval_steps = %EVAL_STEPS
  eval_period = %EVAL_PERIOD
  random_seed = None  # use faster, hardware RNG
  # run_xprof = True  # GOOGLE-INTERNAL
  summarize_config_fn = @gin_utils.summarize_gin_config

partitioning.ModelBasedPjitPartitioner:
  num_partitions = 1
  model_parallel_submesh = ()
# # BEGIN GOOGLE-INTERNAL
# # For internal version, we override the `ModelBasedPjitPartitioner`.
# # TODO(hwchung): Remove this after PjitPartitioner is deprecated.
# train_script.train.partitioner = @partitioning.PjitPartitioner()

# partitioning.PjitPartitioner:
#   num_partitions = 1
#   model_parallel_submesh = ()
#   parameter_partitioning_dims = 1
# # END GOOGLE-INTERNAL

train/utils.DatasetConfig: 
  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'train'
  batch_size = %BATCH_SIZE
  shuffle = True
  seed = None  # use a new seed each run/restart
  use_cached = %USE_CACHED_TASKS
  pack = %PACK
  module = %MIXTURE_OR_TASK_MODULE

train_eval/utils.DatasetConfig:
  mixture_or_task_name = %MIXTURE_OR_TASK_NAME 
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'validation'
  batch_size = %BATCH_SIZE
  shuffle = False
  seed = 42
  use_cached = %USE_CACHED_TASKS
  pack = False
  module = %MIXTURE_OR_TASK_MODULE

INITIAL_CHECKPOINT_PATH = 'asdlfsafsakjdfnsjkafnskjanfkjsdnf'
RESTORE = None


utils.CheckpointConfig:
  restore = %RESTORE  # None or "@utils.RestoreCheckpointConfig()"
  save = @utils.SaveCheckpointConfig()
utils.RestoreCheckpointConfig:
  path = %INITIAL_CHECKPOINT_PATH
  mode = 'specific'
  dtype = 'float32'
utils.SaveCheckpointConfig:
  period = %PERIOD
  dtype = 'float32'
  keep = None  # keep all checkpoints
  save_dataset = False  # don't checkpoint dataset state

trainer.Trainer:
  num_microbatches = None
  learning_rate_fn = @utils.create_learning_rate_scheduler()
utils.create_learning_rate_scheduler:
  factors = %FACTORS
  base_learning_rate = %LEARNING_RATE
  warmup_steps = 10000
